{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyarrow.feather as feather\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "import pyarrow.feather as feather\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = pd.read_csv(\"data/sample.csv\", on_bad_lines='skip')\n",
    "#Loads the sample that is 10% of the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155872"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Cammell Laird.\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = sampled_data[sampled_data['type'] == 'â€ said Linton Roberts']\n",
    "biases1 = biases.reset_index()\n",
    "biases1.content[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sputnik = biases1[biases1['content'] == 'It only takes a few moments to share an article, but the person on the other end who reads it might have his life changed forever.']\n",
    "len(sputnik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reliable': 48.22415265633921, 'fake': 51.775847343660786}\n",
      "635781\n"
     ]
    }
   ],
   "source": [
    "def removeLabels(df): \n",
    "    df.type = df.type.replace({'political': 'reliable', 'junksci': 'fake', 'bias' : 'fake', 'satire': 'fake', 'conspiracy': 'fake', 'rumor': 'fake', 'unreliable' : 'fake', 'clickbait': 'fake', 'hate': 'fake'})\n",
    "    df = df[(df.type == 'reliable') |(df.type == 'fake')]\n",
    "    df = df[df.type.notnull()]\n",
    "    df = df[df.content.notnull()]\n",
    "    df = df.drop_duplicates(subset = 'content', keep = 'last')\n",
    "    df = df.reset_index()\n",
    "    return df \n",
    "\n",
    "def labelperc(df): \n",
    "    labeldict = {}\n",
    "    for i in df.type: \n",
    "        if i in labeldict: \n",
    "            labeldict[i] +=1\n",
    "        else: \n",
    "            labeldict[i] = 1\n",
    "    for i in labeldict: \n",
    "        labeldict[i] = labeldict[i]/len(df)*100\n",
    "    return labeldict\n",
    "cleanSample = removeLabels(sampled_data)\n",
    "print(labelperc(cleanSample)) \n",
    "print(len(cleanSample))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Market News | Thu Nov 3, 2016 | 7:25am EDT BRIEF-The Buckle qtrly sales fell 14.6 pct to $239.2 mln \\nNov 3 Buckle Inc \\n* Q3 revenue view $248.9 million -- Thomson Reuters I/B/E/S \\n* The buckle, inc. Reports october 2016 net sales \\n* Quarterly sales fell 14.6 percent to $239.2 million \\n* October sales fell 15.1 percent to $69.1 million Source text for Eikon: Further company coverage: Next In Market News '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanSample.content[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def checkForDuplicateArticles(df): \n",
    "    articledict = {}\n",
    "    df_sample = df.sample(frac=0.1, random_state=1)\n",
    "    for i in df_sample.content: \n",
    "        if str(i) in articledict: \n",
    "            articledict[str(i)] +=1\n",
    "        else: \n",
    "            articledict[str(i)] = 1\n",
    "    #at this point we have all different contents, and the amount of times we see them\n",
    "    articledict = {key:val for key, val in articledict.items() if val >1}\n",
    "    #here we delete the contents which we only see once\n",
    "\n",
    "    contentsToRemove = list(articledict.keys())\n",
    "    #get a list of the content, we will remove articles for containing\n",
    "    \n",
    "    \n",
    "    return contentsToRemove\n",
    "    \n",
    "wordstoremove = checkForDuplicateArticles(cleanSample)\n",
    "len(wordstoremove)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
