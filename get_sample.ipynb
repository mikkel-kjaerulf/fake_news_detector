{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subset of full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyarrow.feather as feather\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "import pyarrow.feather as feather\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to load a subset of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indhenter et sample på ca. 1,45M artikler, meget tidseffektivt\n",
    "def getSample(csvstring: str, sample_size: int):\n",
    "    random.seed(0)\n",
    "    n = 11000000 #number of records in file (excludes header)\n",
    "    s = sample_size #desired sample size\n",
    "    skip = sorted(random.sample(range(1,n+1),n-s))\n",
    "    # Read the CSV file, skipping the randomly selected rows\n",
    "    sampled_data = pd.read_csv(csvstring, on_bad_lines='skip', skiprows=skip, index_col=0)\n",
    "    sampled_data = sampled_data.reset_index()\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getSample(\"data/news_cleaned_2018_02_13-1.csv\", 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38701"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting The Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_freq(dataframe):\n",
    "    typedict = {}\n",
    "    for i in dataframe['type']: \n",
    "        if str(i) in typedict:\n",
    "            typedict[str(i)] +=1 \n",
    "        else: \n",
    "            typedict[str(i)] =1 \n",
    "    typedict\n",
    "    typedictperc = typedict.copy()\n",
    "    #laver et nyt dictionairy som viser det i procenttal\n",
    "    for i in typedictperc:\n",
    "        typedictperc[i] = (typedictperc[i]/(len(data)))*100\n",
    "    return typedictperc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'satire': 1.3281310560450634,\n",
       " 'hate': 1.0077258985555928,\n",
       " 'conspiracy': 9.723262964781272,\n",
       " 'junksci': 1.3978966951758354,\n",
       " 'fake': 10.6457197488437,\n",
       " 'clickbait': 2.6846851502545155,\n",
       " 'political': 19.219141624247435,\n",
       " 'bias': 13.371747500064599,\n",
       " 'nan': 4.7027208599261,\n",
       " 'unreliable': 3.5167049947029794,\n",
       " 'unknown': 4.3874835275574275,\n",
       " 'reliable': 22.39735407353815,\n",
       " 'rumor': 5.61742590630733}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_type_freq(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reliable': 22.561888537768805,\n",
       " 'political': 28.617108052577617,\n",
       " 'bias': 15.280389897611691,\n",
       " 'Satire': 1.716459421738357,\n",
       " 'fake': 10.905098641191126,\n",
       " 'conspiracy': 10.645397202669349,\n",
       " 'unreliable': 3.7580450222794273,\n",
       " 'clickbait': 3.4334005989277774,\n",
       " 'junksci': 1.703052520039264,\n",
       " 'hate': 1.3791601051965903}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allArticlesCount = 928083 + 146080 + 1300444 + 905981 + 144939 + 117374 + 292201 + 2435471 + 319830 + 1920139\n",
    "#Jeg henter data from README.md om hvor mange artikler af hver type, der er i det fulde datasæt\n",
    "realtypeperc = {\"reliable\": (1920139/allArticlesCount)*100, \"political\": (2435471/allArticlesCount)*100, \"bias\": (1300444/allArticlesCount)*100, \"Satire\": (146080/allArticlesCount)*100, \"fake\": (928083/allArticlesCount)*100, \"conspiracy\":(905981/allArticlesCount)*100, \"unreliable\": (319830/allArticlesCount)*100, \"clickbait\":(292201/allArticlesCount)*100, \"junksci\":(144939/allArticlesCount)*100 , \"hate\":(117374/allArticlesCount)*100}\n",
    "realtypeperc\n",
    "#jeg præsenterer det som procenter\n",
    "#ved sammenligning er det tydeligt, at Mikkels forkortede datasæt ikke har markant anderledes proportion ift. artikeltyper, end det fulde datasæt\n",
    "#Største forskel er at der er tilføjet nye typer af artikler som rumor til datasættet, siden README blev skrevet. \n",
    "#Vi kan bruge dette til at forsikre os selv om at vores forkortede datasæt er repræsentativt for det fulde datasæt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Sample For Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we preprocess there are some articles we can remove from the data set. We also change the labels so that\n",
    "they are either 'reliable' or 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Klassificerer alt som reliable/fake\n",
    "def binary_labels(df): \n",
    "    #klassificerer alle de artikler vi vil bruge ind i reliable eller fake\n",
    "    df.type = df.type.replace({'political': 'reliable', 'junksci': 'fake', 'bias' : 'fake', 'satire': 'fake', 'conspiracy': 'fake', 'rumor': 'fake', 'unreliable' : 'fake', 'clickbait': 'fake', 'hate': 'fake'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = binary_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fjerner alle de artikler vi ikke skal bruges. NB: SKAL kaldes på en dataframe, som allerede har været igennem binary_labels\n",
    "def remove_bad_articles(df): \n",
    "    #fjerner artikler som har volapyk types (inklusiv 'unknown')\n",
    "    df = df[(df.type == 'reliable') |(df.type == 'fake')]\n",
    "    #fjerner artikler som ikke har nogen type\n",
    "    df = df[df.type.notnull()]\n",
    "    #fjerner artikler uden content\n",
    "    df = df[df.content.notnull()]\n",
    "    #fjerner duplerede artikler, ud over en enkelt\n",
    "    df = df.drop_duplicates(subset = 'content', keep = 'last')\n",
    "    #fjerner de artikler som ikke indeholder mindst et latinsk bogstav\n",
    "    df = df[df.content.str.contains('[a-z]')]\n",
    "    #reset index gør, at hvis vi fjerner artikle [2], bliver artikel [3] rykket ned på index [2] osv. dernedad.\n",
    "    df = df.reset_index()\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_bad_articles(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29729"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelperc(df): \n",
    "    labeldict = {}\n",
    "    for i in df.type: \n",
    "        if i in labeldict: \n",
    "            labeldict[i] +=1\n",
    "        else: \n",
    "            labeldict[i] = 1\n",
    "    for i in labeldict: \n",
    "        labeldict[i] = labeldict[i]/len(df)*100\n",
    "    return labeldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 51.82145379932053, 'reliable': 48.17854620067947}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelperc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/sample_structured.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the larger sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we preprocess the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import preprocessing_functions\n",
    "import cleaning_functions\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning took 40.800045013427734 seconds\n",
      "Tokenizing took 9.27118706703186 seconds\n",
      "Removing stopwords took 0.2569248676300049 seconds\n",
      "Stemming took 0.0025777816772460938 seconds\n",
      "Converting to list took 32.61544704437256 seconds\n",
      "writing to csv took 0.5568921566009521 seconds\n",
      "cleaning took 34.458556175231934 seconds\n",
      "Tokenizing took 7.78607177734375 seconds\n",
      "Removing stopwords took 0.15650081634521484 seconds\n",
      "Stemming took 0.0022678375244140625 seconds\n",
      "Converting to list took 26.835575103759766 seconds\n",
      "writing to csv took 0.4476180076599121 seconds\n",
      "cleaning took 31.848845958709717 seconds\n",
      "Tokenizing took 7.79075288772583 seconds\n",
      "Removing stopwords took 0.014870882034301758 seconds\n",
      "Stemming took 0.15406107902526855 seconds\n",
      "Converting to list took 27.532126903533936 seconds\n",
      "writing to csv took 0.469926118850708 seconds\n"
     ]
    }
   ],
   "source": [
    "file_name = \"data/sample_preprocessed_ver_\" + datetime.today().strftime('%Y-%m-%d-%s') + \".csv\"\n",
    "for chunck in pd.read_csv(\"data/sample_structured.csv\", chunksize=10000, index_col=0):\n",
    "    cleaning_functions.clean_dataframe(chunck)\n",
    "    preprocessing_functions.preprocess(chunck)\n",
    "    start = time.time()\n",
    "    chunck.to_csv(file_name, mode='a')\n",
    "    end = time.time()\n",
    "    print(\"writing to csv took \" + str(end - start) + \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
