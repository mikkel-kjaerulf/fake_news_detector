{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subset of full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyarrow.feather as feather\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "import pyarrow.feather as feather\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to load a subset of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indhenter et sample på ca. 1,45M artikler, meget tidseffektivt\n",
    "def getSample(csvstring: str, sample_size: int):\n",
    "    random.seed(0)\n",
    "    n = 11000000 #number of records in file (excludes header)\n",
    "    s = sample_size #desired sample size\n",
    "    skip = sorted(random.sample(range(1,n+1),n-s))\n",
    "    # Read the CSV file, skipping the randomly selected rows\n",
    "    sampled_data = pd.read_csv(csvstring, on_bad_lines='skip', skiprows=skip, index_col=0)\n",
    "    sampled_data = sampled_data.reset_index()\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gj/r8x5s5mn7jg12hs3c28wpg4c0000gn/T/ipykernel_32236/4167401217.py:8: DtypeWarning: Columns (1,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sampled_data = pd.read_csv(csvstring, on_bad_lines='skip', skiprows=skip, index_col=0)\n"
     ]
    }
   ],
   "source": [
    "data = getSample(\"data/news_cleaned_2018_02_13-1.csv\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77430"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting The Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_freq(dataframe):\n",
    "    typedict = {}\n",
    "    for i in dataframe['type']: \n",
    "        if str(i) in typedict:\n",
    "            typedict[str(i)] +=1 \n",
    "        else: \n",
    "            typedict[str(i)] =1 \n",
    "    typedict\n",
    "    typedictperc = typedict.copy()\n",
    "    #laver et nyt dictionairy som viser det i procenttal\n",
    "    for i in typedictperc:\n",
    "        typedictperc[i] = (typedictperc[i]/(len(data)))*100\n",
    "    return typedictperc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unreliable': 3.4844375565026473,\n",
       " 'satire': 1.312152912307891,\n",
       " 'hate': 0.9414955443626501,\n",
       " 'conspiracy': 9.816608549657756,\n",
       " 'political': 19.45886607258169,\n",
       " 'junksci': 1.3780188557406692,\n",
       " 'fake': 10.515304145679968,\n",
       " 'bias': 13.462482242025056,\n",
       " 'reliable': 22.384088854449182,\n",
       " 'clickbait': 2.6940462353093118,\n",
       " 'rumor': 5.453958414051401,\n",
       " 'nan': 4.68035645098799,\n",
       " 'unknown': 4.416892677256877,\n",
       " '2018-02-10 13:43:39.521661': 0.0012914890869172157}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_type_freq(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reliable': 22.561888537768805,\n",
       " 'political': 28.617108052577617,\n",
       " 'bias': 15.280389897611691,\n",
       " 'Satire': 1.716459421738357,\n",
       " 'fake': 10.905098641191126,\n",
       " 'conspiracy': 10.645397202669349,\n",
       " 'unreliable': 3.7580450222794273,\n",
       " 'clickbait': 3.4334005989277774,\n",
       " 'junksci': 1.703052520039264,\n",
       " 'hate': 1.3791601051965903}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allArticlesCount = 928083 + 146080 + 1300444 + 905981 + 144939 + 117374 + 292201 + 2435471 + 319830 + 1920139\n",
    "#Jeg henter data from README.md om hvor mange artikler af hver type, der er i det fulde datasæt\n",
    "realtypeperc = {\"reliable\": (1920139/allArticlesCount)*100, \"political\": (2435471/allArticlesCount)*100, \"bias\": (1300444/allArticlesCount)*100, \"Satire\": (146080/allArticlesCount)*100, \"fake\": (928083/allArticlesCount)*100, \"conspiracy\":(905981/allArticlesCount)*100, \"unreliable\": (319830/allArticlesCount)*100, \"clickbait\":(292201/allArticlesCount)*100, \"junksci\":(144939/allArticlesCount)*100 , \"hate\":(117374/allArticlesCount)*100}\n",
    "realtypeperc\n",
    "#jeg præsenterer det som procenter\n",
    "#ved sammenligning er det tydeligt, at Mikkels forkortede datasæt ikke har markant anderledes proportion ift. artikeltyper, end det fulde datasæt\n",
    "#Største forskel er at der er tilføjet nye typer af artikler som rumor til datasættet, siden README blev skrevet. \n",
    "#Vi kan bruge dette til at forsikre os selv om at vores forkortede datasæt er repræsentativt for det fulde datasæt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Sample For Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we preprocess there are some articles we can remove from the data set. We also change the labels so that\n",
    "they are either 'reliable' or 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Klassificerer alt som reliable/fake\n",
    "def binary_labels(df): \n",
    "    #klassificerer alle de artikler vi vil bruge ind i reliable eller fake\n",
    "    df.type = df.type.replace({'political': 'reliable', 'junksci': 'fake', 'bias' : 'fake', 'satire': 'fake', 'conspiracy': 'fake', 'rumor': 'fake', 'unreliable' : 'fake', 'clickbait': 'fake', 'hate': 'fake'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = binary_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fjerner alle de artikler vi ikke skal bruges. NB: SKAL kaldes på en dataframe, som allerede har været igennem binary_labels\n",
    "def remove_bad_articles(df): \n",
    "    #fjerner artikler som har volapyk types (inklusiv 'unknown')\n",
    "    df = df[(df.type == 'reliable') |(df.type == 'fake')]\n",
    "    #fjerner artikler som ikke har nogen type\n",
    "    df = df[df.type.notnull()]\n",
    "    #fjerner artikler uden content\n",
    "    df = df[df.content.notnull()]\n",
    "    #fjerner duplerede artikler, ud over en enkelt\n",
    "    df = df.drop_duplicates(subset = 'content', keep = 'last')\n",
    "    #fjerner de artikler som ikke indeholder mindst et latinsk bogstav\n",
    "    df = df[df.content.str.contains('[a-z]')]\n",
    "    #reset index gør, at hvis vi fjerner artikle [2], bliver artikel [3] rykket ned på index [2] osv. dernedad.\n",
    "    df = df.reset_index()\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_bad_articles(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59165"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelperc(df): \n",
    "    labeldict = {}\n",
    "    for i in df.type: \n",
    "        if i in labeldict: \n",
    "            labeldict[i] +=1\n",
    "        else: \n",
    "            labeldict[i] = 1\n",
    "    for i in labeldict: \n",
    "        labeldict[i] = labeldict[i]/len(df)*100\n",
    "    return labeldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 51.54398715456774, 'reliable': 48.45601284543227}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelperc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/sample_structured.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the larger sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we preprocess the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import preprocessing_functions\n",
    "import cleaning_functions\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning took 39.1953649520874 seconds\n",
      "Tokenizing took 8.940755844116211 seconds\n",
      "Removing stopwords took 0.03564906120300293 seconds\n",
      "Stemming took 0.28874802589416504 seconds\n",
      "Converting to list took 31.445418119430542 seconds\n",
      "writing to csv took 0.5369808673858643 seconds\n",
      "cleaning took 40.923473834991455 seconds\n",
      "Tokenizing took 9.286044836044312 seconds\n",
      "Removing stopwords took 0.01805400848388672 seconds\n",
      "Stemming took 0.0022308826446533203 seconds\n",
      "Converting to list took 32.17430830001831 seconds\n",
      "writing to csv took 0.5546891689300537 seconds\n",
      "cleaning took 36.35886788368225 seconds\n",
      "Tokenizing took 8.207499027252197 seconds\n",
      "Removing stopwords took 0.18811607360839844 seconds\n",
      "Stemming took 0.0019729137420654297 seconds\n",
      "Converting to list took 28.507335901260376 seconds\n",
      "writing to csv took 0.4858560562133789 seconds\n",
      "cleaning took 32.03664016723633 seconds\n",
      "Tokenizing took 7.440240144729614 seconds\n",
      "Removing stopwords took 0.16337108612060547 seconds\n",
      "Stemming took 0.0022242069244384766 seconds\n",
      "Converting to list took 25.265047073364258 seconds\n",
      "writing to csv took 0.45786285400390625 seconds\n",
      "cleaning took 28.80881404876709 seconds\n",
      "Tokenizing took 7.614372730255127 seconds\n",
      "Removing stopwords took 0.021365880966186523 seconds\n",
      "Stemming took 0.16286087036132812 seconds\n",
      "Converting to list took 27.016571044921875 seconds\n",
      "writing to csv took 0.4726221561431885 seconds\n",
      "cleaning took 33.18356895446777 seconds\n",
      "Tokenizing took 7.697580099105835 seconds\n",
      "Removing stopwords took 0.03235888481140137 seconds\n",
      "Stemming took 0.16254806518554688 seconds\n",
      "Converting to list took 27.70731806755066 seconds\n",
      "writing to csv took 0.4821310043334961 seconds\n"
     ]
    }
   ],
   "source": [
    "file_name = \"data/sample_preprocessed_ver_\" + datetime.today().strftime('%Y-%m-%d-%s') + \".csv\"\n",
    "for chunck in pd.read_csv(\"data/sample_structured.csv\", chunksize=10000, index_col=0):\n",
    "    cleaning_functions.clean_dataframe(chunck)\n",
    "    preprocessing_functions.preprocess(chunck)\n",
    "    start = time.time()\n",
    "    chunck.to_csv(file_name, mode='a')\n",
    "    end = time.time()\n",
    "    print(\"writing to csv took \" + str(end - start) + \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
