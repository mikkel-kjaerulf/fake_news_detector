{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subset of full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyarrow.feather as feather\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "import pyarrow.feather as feather\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to load a subset of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indhenter et sample på ca. 1,45M artikler, meget tidseffektivt\n",
    "def getSample(csvstring: str, sample_size: int):\n",
    "    random.seed(0)\n",
    "    n = 11000000 #number of records in file (excludes header)\n",
    "    s = sample_size #desired sample size\n",
    "    skip = sorted(random.sample(range(1,n+1),n-s))\n",
    "    # Read the CSV file, skipping the randomly selected rows\n",
    "    sampled_data = pd.read_csv(csvstring, on_bad_lines='skip', skiprows=skip, index_col=0)\n",
    "    sampled_data = sampled_data.reset_index()\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getSample(\"data/news_cleaned_2018_02_13-1.csv\",1900000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7748"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting The Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_freq(dataframe):\n",
    "    typedict = {}\n",
    "    for i in dataframe['type']: \n",
    "        if str(i) in typedict:\n",
    "            typedict[str(i)] +=1 \n",
    "        else: \n",
    "            typedict[str(i)] =1 \n",
    "    typedict\n",
    "    typedictperc = typedict.copy()\n",
    "    #laver et nyt dictionairy som viser det i procenttal\n",
    "    for i in typedictperc:\n",
    "        typedictperc[i] = (typedictperc[i]/(len(data)))*100\n",
    "    return typedictperc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 10.441404233350541,\n",
       " 'junksci': 1.2906556530717606,\n",
       " 'political': 18.856479091378418,\n",
       " 'conspiracy': 9.537945276200311,\n",
       " 'nan': 4.981930820856995,\n",
       " 'bias': 13.461538461538462,\n",
       " 'clickbait': 2.8523489932885906,\n",
       " 'unreliable': 3.523489932885906,\n",
       " 'rumor': 5.846670108415076,\n",
       " 'unknown': 4.168817759421787,\n",
       " 'reliable': 22.676819824470833,\n",
       " 'hate': 1.0712441920495612,\n",
       " 'satire': 1.2906556530717606}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_type_freq(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reliable': 22.561888537768805,\n",
       " 'political': 28.617108052577617,\n",
       " 'bias': 15.280389897611691,\n",
       " 'Satire': 1.716459421738357,\n",
       " 'fake': 10.905098641191126,\n",
       " 'conspiracy': 10.645397202669349,\n",
       " 'unreliable': 3.7580450222794273,\n",
       " 'clickbait': 3.4334005989277774,\n",
       " 'junksci': 1.703052520039264,\n",
       " 'hate': 1.3791601051965903}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allArticlesCount = 928083 + 146080 + 1300444 + 905981 + 144939 + 117374 + 292201 + 2435471 + 319830 + 1920139\n",
    "#Jeg henter data from README.md om hvor mange artikler af hver type, der er i det fulde datasæt\n",
    "realtypeperc = {\"reliable\": (1920139/allArticlesCount)*100, \"political\": (2435471/allArticlesCount)*100, \"bias\": (1300444/allArticlesCount)*100, \"Satire\": (146080/allArticlesCount)*100, \"fake\": (928083/allArticlesCount)*100, \"conspiracy\":(905981/allArticlesCount)*100, \"unreliable\": (319830/allArticlesCount)*100, \"clickbait\":(292201/allArticlesCount)*100, \"junksci\":(144939/allArticlesCount)*100 , \"hate\":(117374/allArticlesCount)*100}\n",
    "realtypeperc\n",
    "#jeg præsenterer det som procenter\n",
    "#ved sammenligning er det tydeligt, at Mikkels forkortede datasæt ikke har markant anderledes proportion ift. artikeltyper, end det fulde datasæt\n",
    "#Største forskel er at der er tilføjet nye typer af artikler som rumor til datasættet, siden README blev skrevet. \n",
    "#Vi kan bruge dette til at forsikre os selv om at vores forkortede datasæt er repræsentativt for det fulde datasæt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Sample For Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we preprocess there are some articles we can remove from the data set. We also change the labels so that\n",
    "they are either 'reliable' or 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Klassificerer alt som reliable/fake\n",
    "def binary_labels(df): \n",
    "    #klassificerer alle de artikler vi vil bruge ind i reliable eller fake\n",
    "    df.type = df.type.replace({'political': 'reliable', 'junksci': 'fake', 'bias' : 'fake', 'satire': 'fake', 'conspiracy': 'fake', 'rumor': 'fake', 'unreliable' : 'fake', 'clickbait': 'fake', 'hate': 'fake'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = binary_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fjerner alle de artikler vi ikke skal bruges. NB: SKAL kaldes på en dataframe, som allerede har været igennem binary_labels\n",
    "def remove_bad_articles(df): \n",
    "    #fjerner artikler som har volapyk types (inklusiv 'unknown')\n",
    "    df = df[(df.type == 'reliable') |(df.type == 'fake')]\n",
    "    #fjerner artikler som ikke har nogen type\n",
    "    df = df[df.type.notnull()]\n",
    "    #fjerner artikler uden content\n",
    "    df = df[df.content.notnull()]\n",
    "    #fjerner duplerede artikler, ud over en enkelt\n",
    "    df = df.drop_duplicates(subset = 'content', keep = 'last')\n",
    "    #fjerner de artikler som ikke indeholder mindst et latinsk bogstav\n",
    "    df = df[df.content.str.contains('[a-z]')]\n",
    "    #reset index gør, at hvis vi fjerner artikle [2], bliver artikel [3] rykket ned på index [2] osv. dernedad.\n",
    "    df = df.reset_index()\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_bad_articles(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6071"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelperc(df): \n",
    "    labeldict = {}\n",
    "    for i in df.type: \n",
    "        if i in labeldict: \n",
    "            labeldict[i] +=1\n",
    "        else: \n",
    "            labeldict[i] = 1\n",
    "    for i in labeldict: \n",
    "        labeldict[i] = labeldict[i]/len(df)*100\n",
    "    return labeldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 52.18250700049415, 'reliable': 47.817492999505845}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelperc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"data/sample_structured.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the larger sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we preprocess the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import preprocessing_functions\n",
    "import cleaning_functions\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58ed371bd96482eb36b9e7eb1e318c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=6071)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name = \"data/sample_preprocessed_ver_\" + datetime.today().strftime('%Y-%m-%d-%s') + \".csv\"\n",
    "meta_data = \"meta_data/preprocess_info_\" + datetime.today().strftime('%Y-%m-%d-%s') + \"csv\"\n",
    "f = IntProgress(min=0, max=len(data))\n",
    "display(f)\n",
    "CHUNK_SIZE = 10000\n",
    "for chunk in pd.read_csv(temp, chunksize=CHUNK_SIZE, index_col=0):\n",
    "    #chunk = chunk[['type', 'content']]\n",
    "    cleaning_functions.clean_dataframe(chunk) # clean chunk\n",
    "    chunk['content'] = chunk['content'].apply(preprocessing_functions.tokenizer()) # tokenize    \n",
    "    #vocab = set(itertools.chain.from_iterable(chunk['content'])) # get vocabulary\n",
    "    chunk['content'] = chunk['content'].apply(preprocessing_functions.stopwords_remover()) # remove stopwords\n",
    "    #vocab_no_stopwords = set(itertools.chain.from_iterable(chunk['content'])) # get vocabulary\n",
    "    chunk['content'] = chunk['content'].apply(preprocessing_functions.token_stemmer()) # stem tokens\n",
    "    #vocab_stemmed = set(itertools.chain.from_iterable(chunk['content'])) # get vocabulary\n",
    "    chunk.to_csv(file_name, mode=\"a\")\n",
    "\n",
    "    #with open(\"data/content_test.csv\", 'a') as file:\n",
    "    #    writer = csv.writer(file)\n",
    "    #    writer.writerows(chunk['content'])\n",
    "\n",
    "    #chunk.to_csv(\"data/type_test.csv\", mode=\"a\", columns=['type'])\n",
    "\n",
    "    #with open(meta_data,'a') as meta_data_file:\n",
    "    #    writer = csv.writer(meta_data_file)\n",
    "    #    writer.writerow([(len(vocab_no_stopwords) / len(vocab)),(len(vocab_no_stopwords) / len(vocab_stemmed))])\n",
    "\n",
    "    f.value += CHUNK_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d82b2e002de3f5f2748faab7eed39a54b7eae736eec0eed65c518ce11052f6ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
