{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "import pyarrow.feather as feather\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"([\\d]{1,2}[\\/|\\-][\\d]{1,2}(?:[\\/|\\-][\\d]{2,4})?|[\\d]{2,4}[\\/|\\-][\\d]{1,2}[\\/|\\-][\\d]{1,2}|(?:january|february|march|april|may|june|july|august|september|october|november|december)[\\s][\\d]{1,2}[a-z][a-z](?:\\s[\\d]{2,4})|[\\d][\\d]\\w?\\w?\\sof\\s(?:january|february|march|april|may|june|july|august|september|october|november|december)(?:\\s[\\d]{2,4})?|(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s\\d\\d?\\w?\\w?,?(?:\\s\\d{2,4})?)\")\n",
    "def clean_string(st):\n",
    "    s1 = pattern.sub(\"date\",st)\n",
    "    return clean(s1, lower=True,\n",
    "                    no_line_breaks=True,\n",
    "                    no_emails=True,\n",
    "                    no_urls=True,\n",
    "                    no_numbers=True,\n",
    "                    no_punct=True,\n",
    "                    lang=\"en\",\n",
    "                    replace_with_number=\"num\",\n",
    "                    replace_with_email=\"email\",\n",
    "                    replace_with_url=\"url\")\n",
    "            \n",
    "\n",
    "def clean_dataframe(dataframe):\n",
    "    start = time.time()\n",
    "    dataframe['content'] = dataframe['content'].apply(clean_string)\n",
    "    end = time.time()\n",
    "    print(\"cleaning took \" + str(end - start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english_stopwords(stopwords):\n",
    "    def remove_stopwords(tokenlist):\n",
    "        return filter(lambda x : x not in stopwords, tokenlist)\n",
    "    return remove_stopwords\n",
    "\n",
    "def stem_tokens():\n",
    "    stemmer = PorterStemmer()\n",
    "    def stem_tokenlist(tokenlist):\n",
    "        return map(stemmer.stem, tokenlist)\n",
    "    return stem_tokenlist\n",
    "\n",
    "def tokenize():\n",
    "    def tokenize_text(s):\n",
    "        return list((map(nltk.word_tokenize, s)))\n",
    "    return tokenize_text\n",
    "\n",
    "def to_list():\n",
    "    def turn_to_list(it):\n",
    "        return list(it)\n",
    "    return turn_to_list\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    # tokenize content column\n",
    "    #print(\"Tokenizing...\")\n",
    "    start = time.time()\n",
    "    dataframe['content'] = dataframe['content'].apply(nltk.word_tokenize)\n",
    "    end = time.time()\n",
    "    print(\"tokenizing took \" + str(end - start) + \" seconds\")\n",
    "    #tokens = list(itertools.chain.from_iterable(dataframe['content']))\n",
    "    #vocabulary = set(tokens)\n",
    "    # remove stopwords\n",
    "    start = time.time()\n",
    "    dataframe['content'] = dataframe['content'].apply(remove_english_stopwords(stopwords.words('english')))\n",
    "    end = time.time()\n",
    "    print(\"removing stopwords took \" + str(end - start) + \" seconds\")\n",
    "    #tokens_no_stopwords = list(itertools.chain.from_iterable(dataframe['content']))\n",
    "    #vocabulary_no_stopwords = set(tokens_no_stopwords)\n",
    "    #print(\"Reduction rate of removing stopwords: \" + str(1 - len(vocabulary_no_stopwords) / len(vocabulary)))\n",
    "    # stem tokens\n",
    "    start = time.time()\n",
    "    dataframe['content'] = dataframe['content'].apply(stem_tokens())\n",
    "    end = time.time()\n",
    "    print(\"stemming took \" + str(end - start) + \" seconds\")\n",
    "\n",
    "    start = time.time()\n",
    "    dataframe['content'] = dataframe['content'].apply(to_list())\n",
    "    end = time.time()\n",
    "    print(\"converting to list took\" + str(end - start) + \" seconds\")\n",
    "\n",
    "    #tokens_stem = list(itertools.chain.from_iterable(dataframe['content']))\n",
    "    #print(\"Stemmed tokens = \" + str(tokens_stem))\n",
    "    #vocabulary_stem = set(tokens_stem)\n",
    "    #print(\"Reduction rate of stemming: \" + str(1 - len(vocabulary_stem)/len(vocabulary_no_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x36406de10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_tokens()([\"running\", \"runs\", \"running\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"([\\d]{1,2}[\\/|\\-][\\d]{1,2}(?:[\\/|\\-][\\d]{2,4})?|[\\d]{2,4}[\\/|\\-][\\d]{1,2}[\\/|\\-][\\d]{1,2}|(?:january|february|march|april|may|june|july|august|september|october|november|december)[\\s][\\d]{1,2}[a-z][a-z](?:\\s[\\d]{2,4})|[\\d][\\d]\\w?\\w?\\sof\\s(?:january|february|march|april|may|june|july|august|september|october|november|december)(?:\\s[\\d]{2,4})?|(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s\\d\\d?\\w?\\w?,?(?:\\s\\d{2,4})?)\")\n",
    "stemmer = PorterStemmer()\n",
    "def process_string(s):\n",
    "    s1 = pattern.sub(\"date\",s)\n",
    "    cleaned_string = clean(s1, lower=True,\n",
    "                no_line_breaks=True,\n",
    "                no_emails=True,\n",
    "                no_urls=True,\n",
    "                no_numbers=True,\n",
    "                lang=\"en\",\n",
    "                replace_with_number=\"num\",\n",
    "                replace_with_email=\"email\",\n",
    "                replace_with_url=\"url\")\n",
    "    #print(\"tokenizing...\")\n",
    "    tokens = nltk.word_tokenize(cleaned_string)\n",
    "    #print(\"removing stopwords...\")\n",
    "    tokens_no_stopwords = filter(lambda x : x not in stopwords.words('english'), tokens)\n",
    "    #print(\"stemming...\")\n",
    "    stem_tokens = list(map(stemmer.stem, tokens_no_stopwords))\n",
    "    return stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning took 114.18140387535095 seconds\n",
      "tokenizing took 36.51962924003601 seconds\n",
      "removing stopwords took 0.47901391983032227 seconds\n",
      "stemming took 0.2493901252746582 seconds\n",
      "converting to list took2688.595826148987 seconds\n",
      "cleaning took 976.2744419574738 seconds\n",
      "tokenizing took 34.872575998306274 seconds\n",
      "removing stopwords took 0.5000898838043213 seconds\n",
      "stemming took 0.26245570182800293 seconds\n",
      "converting to list took217.90279412269592 seconds\n",
      "cleaning took 116.32085013389587 seconds\n",
      "tokenizing took 35.21644687652588 seconds\n",
      "removing stopwords took 0.48888683319091797 seconds\n",
      "stemming took 0.2568190097808838 seconds\n",
      "converting to list took274.29063296318054 seconds\n",
      "cleaning took 116.10142111778259 seconds\n",
      "tokenizing took 35.42123317718506 seconds\n",
      "removing stopwords took 0.5335240364074707 seconds\n",
      "stemming took 0.264085054397583 seconds\n",
      "converting to list took124.9652771949768 seconds\n",
      "cleaning took 116.1177179813385 seconds\n",
      "tokenizing took 35.55403017997742 seconds\n",
      "removing stopwords took 0.5011391639709473 seconds\n",
      "stemming took 0.2637200355529785 seconds\n",
      "converting to list took123.65752601623535 seconds\n",
      "cleaning took 114.56433582305908 seconds\n",
      "tokenizing took 108.04838514328003 seconds\n",
      "removing stopwords took 0.5048298835754395 seconds\n",
      "stemming took 0.26018595695495605 seconds\n",
      "converting to list took122.25304198265076 seconds\n",
      "cleaning took 115.46972489356995 seconds\n",
      "tokenizing took 35.20500731468201 seconds\n",
      "removing stopwords took 0.5012762546539307 seconds\n",
      "stemming took 0.264711856842041 seconds\n",
      "converting to list took123.43835806846619 seconds\n",
      "cleaning took 662.2976520061493 seconds\n",
      "tokenizing took 35.21125388145447 seconds\n",
      "removing stopwords took 0.5227351188659668 seconds\n",
      "stemming took 0.26985669136047363 seconds\n",
      "converting to list took365.92763781547546 seconds\n",
      "cleaning took 1066.3868119716644 seconds\n",
      "tokenizing took 34.971482038497925 seconds\n",
      "removing stopwords took 0.5377669334411621 seconds\n",
      "stemming took 0.27278637886047363 seconds\n",
      "converting to list took997.4085869789124 seconds\n",
      "cleaning took 116.22780275344849 seconds\n",
      "tokenizing took 3476.772792816162 seconds\n",
      "removing stopwords took 0.525087833404541 seconds\n",
      "stemming took 0.28051304817199707 seconds\n",
      "converting to list took123.40578603744507 seconds\n",
      "cleaning took 783.9738800525665 seconds\n",
      "tokenizing took 1068.2204871177673 seconds\n",
      "removing stopwords took 0.5369620323181152 seconds\n",
      "stemming took 0.26937317848205566 seconds\n",
      "converting to list took511.47003412246704 seconds\n",
      "cleaning took 589.4327201843262 seconds\n",
      "tokenizing took 35.03020191192627 seconds\n",
      "removing stopwords took 0.5215070247650146 seconds\n",
      "stemming took 0.27500176429748535 seconds\n",
      "converting to list took302.46135783195496 seconds\n",
      "cleaning took 7281.797575950623 seconds\n",
      "tokenizing took 5772.924568891525 seconds\n",
      "removing stopwords took 0.5208768844604492 seconds\n",
      "stemming took 0.2630298137664795 seconds\n",
      "converting to list took3387.7871050834656 seconds\n",
      "cleaning took 115.71195411682129 seconds\n",
      "tokenizing took 35.16118502616882 seconds\n",
      "removing stopwords took 0.5213329792022705 seconds\n",
      "stemming took 0.27500224113464355 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m clean_dataframe(chunck)\n\u001b[1;32m      7\u001b[0m \u001b[39m#print(\"preprocessing...\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m preprocess(chunck)\n\u001b[1;32m      9\u001b[0m \u001b[39m#chunck['content'] = chunck['content'].apply(process_string)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m chunck\u001b[39m.\u001b[39mto_csv(file_name, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 46\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstemming took \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(end \u001b[39m-\u001b[39m start) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 46\u001b[0m dataframe[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataframe[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(to_list())\n\u001b[1;32m     47\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconverting to list took\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(end \u001b[39m-\u001b[39m start) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[41], line 19\u001b[0m, in \u001b[0;36mto_list.<locals>.turn_to_list\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mturn_to_list\u001b[39m(it):\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(it)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.10/site-packages/nltk/stem/porter.py:671\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    669\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step1a(stem)\n\u001b[1;32m    670\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step1b(stem)\n\u001b[0;32m--> 671\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step1c(stem)\n\u001b[1;32m    672\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step2(stem)\n\u001b[1;32m    673\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step3(stem)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.10/site-packages/nltk/stem/porter.py:422\u001b[0m, in \u001b[0;36mPorterStemmer._step1c\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moriginal_condition\u001b[39m(stem):\n\u001b[1;32m    420\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_contains_vowel(stem)\n\u001b[0;32m--> 422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_rule_list(\n\u001b[1;32m    423\u001b[0m     word,\n\u001b[1;32m    424\u001b[0m     [\n\u001b[1;32m    425\u001b[0m         (\n\u001b[1;32m    426\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    428\u001b[0m             nltk_condition\n\u001b[1;32m    429\u001b[0m             \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode \u001b[39m==\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mNLTK_EXTENSIONS\n\u001b[1;32m    430\u001b[0m             \u001b[39melse\u001b[39;49;00m original_condition,\n\u001b[1;32m    431\u001b[0m         )\n\u001b[1;32m    432\u001b[0m     ],\n\u001b[1;32m    433\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.10/site-packages/nltk/stem/porter.py:248\u001b[0m, in \u001b[0;36mPorterStemmer._apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[39mreturn\u001b[39;00m word[: \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(suffix)] \u001b[39m+\u001b[39m replacement\n\u001b[0;32m--> 248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_rule_list\u001b[39m(\u001b[39mself\u001b[39m, word, rules):\n\u001b[1;32m    249\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Applies the first applicable suffix-removal rule to the word\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[39m    Takes a word and a list of suffix-removal rules represented as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m    or None if the rule is unconditional.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39mfor\u001b[39;00m rule \u001b[39min\u001b[39;00m rules:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_name = \"data/sample_preprocessed_no_punct.csv\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    pass\n",
    "for chunck in pd.read_csv(\"data/sample_STRUCTURED.csv\", chunksize=50000):\n",
    "    #print(\"cleaning...\")\n",
    "    clean_dataframe(chunck)\n",
    "    #print(\"preprocessing...\")\n",
    "    preprocess(chunck)\n",
    "    #chunck['content'] = chunck['content'].apply(process_string)\n",
    "    chunck.to_csv(file_name, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gj/r8x5s5mn7jg12hs3c28wpg4c0000gn/T/ipykernel_3022/3757481934.py:1: DtypeWarning: Columns (1,2,3,13,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sample = pd.read_csv(\"data/sample_preprocessed.csv\")\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(\"data/sample_preprocessed.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11467794</td>\n",
       "      <td>8027</td>\n",
       "      <td>9787368</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
       "      <td>['kasha', '--', 'audrey', 'sission', ',', 'lif...</td>\n",
       "      <td>2018-02-11 00:48:58.787555</td>\n",
       "      <td>2018-02-11 00:14:20.346838</td>\n",
       "      <td>2018-02-11 00:14:20.346871</td>\n",
       "      <td>Paid Notice: Deaths  KASHA, AUDREY SISSION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['KASHA  AUDREY SISSION']</td>\n",
       "      <td>KASHA--Audrey Sission, a lifelong New Yorker, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4591569</td>\n",
       "      <td>8760</td>\n",
       "      <td>3009581</td>\n",
       "      <td>thinkprogress.org</td>\n",
       "      <td>political</td>\n",
       "      <td>https://thinkprogress.org/into-the-valley-of-d...</td>\n",
       "      <td>['``', 'forward', ',', 'light', 'brigad', '!',...</td>\n",
       "      <td>2017-11-18T20:01:27.400599</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>Into The Valley Of Death Rode The 600, Into Th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Climate Change, #Climate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6162754</td>\n",
       "      <td>2521</td>\n",
       "      <td>4251195</td>\n",
       "      <td>truthandaction.org</td>\n",
       "      <td>bias</td>\n",
       "      <td>http://www.truthandaction.org/woman-thrown-off...</td>\n",
       "      <td>['woman', 'thrown', 'plane', 'said', 'hillari'...</td>\n",
       "      <td>2017-11-27T01:15:32.269834</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>Woman Thrown Off Plane When She Said Hillary i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2542246</td>\n",
       "      <td>2019</td>\n",
       "      <td>1769246</td>\n",
       "      <td>ecowatch.com</td>\n",
       "      <td>political</td>\n",
       "      <td>https://www.ecowatch.com/cuban-province-well-o...</td>\n",
       "      <td>['cuban', 'provinc', 'well', 'way', 'num', '%'...</td>\n",
       "      <td>2017-11-10T11:18:44.524042</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>Cuban Province Well on Its Way to 100% Renewab...</td>\n",
       "      <td>Guest Contributor, Sierra Club, Common Dreams,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['featured', 'renewables', 'business', 'cuba']</td>\n",
       "      <td>President Obama’s recent announcement that he ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3753783</td>\n",
       "      <td>4806</td>\n",
       "      <td>2429386</td>\n",
       "      <td>weeklystandard.com</td>\n",
       "      <td>political</td>\n",
       "      <td>http://www.weeklystandard.com/print/the-times-...</td>\n",
       "      <td>['new', 'york', 'time', 'greet', 'deleg', 'fro...</td>\n",
       "      <td>2017-11-13T18:09:27.760857</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>The Times Repeats Itself</td>\n",
       "      <td>To The Scrapbook</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['The Scrapbook']</td>\n",
       "      <td>The New York Times greeted delegates with a fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2 Unnamed: 0.1 Unnamed: 0       id              domain  \\\n",
       "0           0.0     11467794       8027  9787368         nytimes.com   \n",
       "1           1.0      4591569       8760  3009581   thinkprogress.org   \n",
       "2           2.0      6162754       2521  4251195  truthandaction.org   \n",
       "3           3.0      2542246       2019  1769246        ecowatch.com   \n",
       "4           4.0      3753783       4806  2429386  weeklystandard.com   \n",
       "\n",
       "        type                                                url  \\\n",
       "0   reliable  https://query.nytimes.com/gst/fullpage.html?re...   \n",
       "1  political  https://thinkprogress.org/into-the-valley-of-d...   \n",
       "2       bias  http://www.truthandaction.org/woman-thrown-off...   \n",
       "3  political  https://www.ecowatch.com/cuban-province-well-o...   \n",
       "4  political  http://www.weeklystandard.com/print/the-times-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  ['kasha', '--', 'audrey', 'sission', ',', 'lif...   \n",
       "1  ['``', 'forward', ',', 'light', 'brigad', '!',...   \n",
       "2  ['woman', 'thrown', 'plane', 'said', 'hillari'...   \n",
       "3  ['cuban', 'provinc', 'well', 'way', 'num', '%'...   \n",
       "4  ['new', 'york', 'time', 'greet', 'deleg', 'fro...   \n",
       "\n",
       "                   scraped_at                 inserted_at  \\\n",
       "0  2018-02-11 00:48:58.787555  2018-02-11 00:14:20.346838   \n",
       "1  2017-11-18T20:01:27.400599  2018-02-07 23:39:33.852671   \n",
       "2  2017-11-27T01:15:32.269834  2018-02-07 23:39:33.852671   \n",
       "3  2017-11-10T11:18:44.524042  2018-02-07 23:39:33.852671   \n",
       "4  2017-11-13T18:09:27.760857  2018-02-07 23:39:33.852671   \n",
       "\n",
       "                   updated_at  \\\n",
       "0  2018-02-11 00:14:20.346871   \n",
       "1  2018-02-07 23:39:33.852696   \n",
       "2  2018-02-07 23:39:33.852696   \n",
       "3  2018-02-07 23:39:33.852696   \n",
       "4  2018-02-07 23:39:33.852696   \n",
       "\n",
       "                                               title  \\\n",
       "0         Paid Notice: Deaths  KASHA, AUDREY SISSION   \n",
       "1  Into The Valley Of Death Rode The 600, Into Th...   \n",
       "2  Woman Thrown Off Plane When She Said Hillary i...   \n",
       "3  Cuban Province Well on Its Way to 100% Renewab...   \n",
       "4                           The Times Repeats Itself   \n",
       "\n",
       "                                             authors keywords  \\\n",
       "0                                                NaN      NaN   \n",
       "1                                                NaN      NaN   \n",
       "2                                                NaN      NaN   \n",
       "3  Guest Contributor, Sierra Club, Common Dreams,...      NaN   \n",
       "4                                   To The Scrapbook      NaN   \n",
       "\n",
       "                                    meta_keywords  \\\n",
       "0                       ['KASHA  AUDREY SISSION']   \n",
       "1                                            ['']   \n",
       "2                                            ['']   \n",
       "3  ['featured', 'renewables', 'business', 'cuba']   \n",
       "4                               ['The Scrapbook']   \n",
       "\n",
       "                                    meta_description  \\\n",
       "0  KASHA--Audrey Sission, a lifelong New Yorker, ...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  President Obama’s recent announcement that he ...   \n",
       "4  The New York Times greeted delegates with a fr...   \n",
       "\n",
       "                        tags summary   source  \n",
       "0                        NaN     NaN  nytimes  \n",
       "1  #Climate Change, #Climate     NaN      NaN  \n",
       "2                        NaN     NaN      NaN  \n",
       "3                        NaN     NaN      NaN  \n",
       "4                        NaN     NaN      NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>929394</th>\n",
       "      <td>929376.0</td>\n",
       "      <td>1658197</td>\n",
       "      <td>7865</td>\n",
       "      <td>849316</td>\n",
       "      <td>dailykos.com</td>\n",
       "      <td>political</td>\n",
       "      <td>https://www.dailykos.com/stories/2017/01/25/16...</td>\n",
       "      <td>['u.s.', 'naval', 'base', 'guantanamo', 'bay',...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Spokesman denies reports that Trump regime pla...</td>\n",
       "      <td>Backgroundurl Avatar_Large, Nickname, Joined, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929395</th>\n",
       "      <td>929377.0</td>\n",
       "      <td>880814</td>\n",
       "      <td>6934</td>\n",
       "      <td>690264</td>\n",
       "      <td>ecowatch.com</td>\n",
       "      <td>political</td>\n",
       "      <td>https://www.ecowatch.com/rooftop-solar-provide...</td>\n",
       "      <td>['cowri', 'collect', 'member', 'particip', 'ti...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Rooftop Solar Provides Net Benefits to All Nev...</td>\n",
       "      <td>Natural Resources Defense Council, Yes, The Co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['energy', 'renewables', 'featured']</td>\n",
       "      <td>The Natural Resources Defense Council (NRDC) a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929396</th>\n",
       "      <td>929378.0</td>\n",
       "      <td>11467044</td>\n",
       "      <td>7277</td>\n",
       "      <td>9786618</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>https://www.nytimes.com/2016/06/13/sports/hock...</td>\n",
       "      <td>['murray', ',', 'num', ',', 'ad', ':', '``', '...</td>\n",
       "      <td>2018-02-11 00:48:58.399133</td>\n",
       "      <td>2018-02-11 00:14:20.346838</td>\n",
       "      <td>2018-02-11 00:14:20.346871</td>\n",
       "      <td>Penguins Finish Off Sharks to Win Stanley Cup</td>\n",
       "      <td>David Pollak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Hockey  Ice', 'Stanley Cup', 'Playoff Games'...</td>\n",
       "      <td>Pittsburgh won its second Stanley Cup in eight...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929397</th>\n",
       "      <td>929379.0</td>\n",
       "      <td>6510986</td>\n",
       "      <td>753</td>\n",
       "      <td>4640342</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>https://www.express.co.uk/showbiz/tv-radio/726...</td>\n",
       "      <td>['num-year-old', 'actor', ',', 'former', 'east...</td>\n",
       "      <td>2017-11-27T01:14:33.570665</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>I’m A Celebrity 2016: Is Larry Lamb joining th...</td>\n",
       "      <td>Rory O'Connor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>LARRY LAMB is the latest celebrity to have agr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929398</th>\n",
       "      <td>929380.0</td>\n",
       "      <td>6794053</td>\n",
       "      <td>459</td>\n",
       "      <td>4934743</td>\n",
       "      <td>dailykos.com</td>\n",
       "      <td>political</td>\n",
       "      <td>https://www.dailykos.com/news/SameSexBinationa...</td>\n",
       "      <td>['leandra', 'english', 'file', 'suit', 'seek',...</td>\n",
       "      <td>2017-11-27T01:14:21.395055</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>Daily Kos: SameSexBinationalCouples</td>\n",
       "      <td>Happy Cog Studios - Http, Www.Happycog.Com, Da...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Next</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.2 Unnamed: 0.1 Unnamed: 0       id         domain  \\\n",
       "929394      929376.0      1658197       7865   849316   dailykos.com   \n",
       "929395      929377.0       880814       6934   690264   ecowatch.com   \n",
       "929396      929378.0     11467044       7277  9786618    nytimes.com   \n",
       "929397      929379.0      6510986        753  4640342  express.co.uk   \n",
       "929398      929380.0      6794053        459  4934743   dailykos.com   \n",
       "\n",
       "             type                                                url  \\\n",
       "929394  political  https://www.dailykos.com/stories/2017/01/25/16...   \n",
       "929395  political  https://www.ecowatch.com/rooftop-solar-provide...   \n",
       "929396   reliable  https://www.nytimes.com/2016/06/13/sports/hock...   \n",
       "929397      rumor  https://www.express.co.uk/showbiz/tv-radio/726...   \n",
       "929398  political  https://www.dailykos.com/news/SameSexBinationa...   \n",
       "\n",
       "                                                  content  \\\n",
       "929394  ['u.s.', 'naval', 'base', 'guantanamo', 'bay',...   \n",
       "929395  ['cowri', 'collect', 'member', 'particip', 'ti...   \n",
       "929396  ['murray', ',', 'num', ',', 'ad', ':', '``', '...   \n",
       "929397  ['num-year-old', 'actor', ',', 'former', 'east...   \n",
       "929398  ['leandra', 'english', 'file', 'suit', 'seek',...   \n",
       "\n",
       "                        scraped_at                 inserted_at  \\\n",
       "929394  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "929395  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "929396  2018-02-11 00:48:58.399133  2018-02-11 00:14:20.346838   \n",
       "929397  2017-11-27T01:14:33.570665  2018-02-07 23:39:33.852671   \n",
       "929398  2017-11-27T01:14:21.395055  2018-02-07 23:39:33.852671   \n",
       "\n",
       "                        updated_at  \\\n",
       "929394  2018-02-02 01:19:41.756664   \n",
       "929395  2018-02-02 01:19:41.756664   \n",
       "929396  2018-02-11 00:14:20.346871   \n",
       "929397  2018-02-07 23:39:33.852696   \n",
       "929398  2018-02-07 23:39:33.852696   \n",
       "\n",
       "                                                    title  \\\n",
       "929394  Spokesman denies reports that Trump regime pla...   \n",
       "929395  Rooftop Solar Provides Net Benefits to All Nev...   \n",
       "929396      Penguins Finish Off Sharks to Win Stanley Cup   \n",
       "929397  I’m A Celebrity 2016: Is Larry Lamb joining th...   \n",
       "929398                Daily Kos: SameSexBinationalCouples   \n",
       "\n",
       "                                                  authors keywords  \\\n",
       "929394  Backgroundurl Avatar_Large, Nickname, Joined, ...      NaN   \n",
       "929395  Natural Resources Defense Council, Yes, The Co...      NaN   \n",
       "929396                                       David Pollak      NaN   \n",
       "929397                                      Rory O'Connor      NaN   \n",
       "929398  Happy Cog Studios - Http, Www.Happycog.Com, Da...      NaN   \n",
       "\n",
       "                                            meta_keywords  \\\n",
       "929394                                               ['']   \n",
       "929395               ['energy', 'renewables', 'featured']   \n",
       "929396  ['Hockey  Ice', 'Stanley Cup', 'Playoff Games'...   \n",
       "929397                                               ['']   \n",
       "929398                                               ['']   \n",
       "\n",
       "                                         meta_description  tags summary  \\\n",
       "929394                                                NaN   NaN     NaN   \n",
       "929395  The Natural Resources Defense Council (NRDC) a...   NaN     NaN   \n",
       "929396  Pittsburgh won its second Stanley Cup in eight...   NaN     NaN   \n",
       "929397  LARRY LAMB is the latest celebrity to have agr...   NaN     NaN   \n",
       "929398                                                NaN  Next     NaN   \n",
       "\n",
       "         source  \n",
       "929394      NaN  \n",
       "929395      NaN  \n",
       "929396  nytimes  \n",
       "929397      NaN  \n",
       "929398      NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
