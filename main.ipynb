{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get subset of FakeNewCorpus data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/church-congregation-brings-gift...</td>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Church Congregation Brings Gift to Waitresses ...</td>\n",
       "      <td>Ruth Harris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/awakening-start-here/...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>Zurich Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700</td>\n",
       "      <td>cnnnext.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://www.cnnnext.com/video/18526/never-hike-...</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Never Hike Alone - A Friday the 13th Fan Film ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/elusive-alien-of-the-sea-caught...</td>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Elusive ‘Alien Of The Sea ‘ Caught By Scientis...</td>\n",
       "      <td>Alexander Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>791</td>\n",
       "      <td>bipartisanreport.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>http://bipartisanreport.com/2018/01/21/trumps-...</td>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Trump’s Genius Poll Is Complete &amp; The Results ...</td>\n",
       "      <td>Gloria Christie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                domain        type  \\\n",
       "0  141               awm.com  unreliable   \n",
       "1  256     beforeitsnews.com        fake   \n",
       "2  700           cnnnext.com  unreliable   \n",
       "3  768               awm.com  unreliable   \n",
       "4  791  bipartisanreport.com   clickbait   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://awm.com/church-congregation-brings-gift...   \n",
       "1  http://beforeitsnews.com/awakening-start-here/...   \n",
       "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
       "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
       "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sometimes the power of Christmas will make you...   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3  When a rare shark was caught, scientists were ...   \n",
       "4  Donald Trump has the unnerving ability to abil...   \n",
       "\n",
       "                   scraped_at                 inserted_at  \\\n",
       "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                   updated_at  \\\n",
       "0  2018-02-02 01:19:41.756664   \n",
       "1  2018-02-02 01:19:41.756664   \n",
       "2  2018-02-02 01:19:41.756664   \n",
       "3  2018-02-02 01:19:41.756664   \n",
       "4  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                               title          authors  \\\n",
       "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
       "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
       "3  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
       "4  Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
       "\n",
       "   keywords meta_keywords                                   meta_description  \\\n",
       "0       NaN          ['']                                                NaN   \n",
       "1       NaN          ['']                                                NaN   \n",
       "2       NaN          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
       "3       NaN          ['']                                                NaN   \n",
       "4       NaN          ['']                                                NaN   \n",
       "\n",
       "  tags  summary  \n",
       "0  NaN      NaN  \n",
       "1  NaN      NaN  \n",
       "2  NaN      NaN  \n",
       "3  NaN      NaN  \n",
       "4  NaN      NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv',index_col=0)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform basic cleaning: remove urls, dates, numbers, emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sometimes the power of christmas will make you...\n",
       "1    awakening of num strands of dna - \"reconnectin...\n",
       "2    never hike alone: a friday the 13th fan film u...\n",
       "3    when a rare shark was caught, scientists were ...\n",
       "4    donald trump has the unnerving ability to abil...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw_data.copy(deep = True)\n",
    "date_pattern = re.compile(r\"([\\d]{1,2}[\\/|\\-][\\d]{1,2}(?:[\\/|\\-][\\d]{2,4})?|[\\d]{2,4}[\\/|\\-][\\d]{1,2}[\\/|\\-][\\d]{1,2}|(?:january|february|march|april|may|june|july|august|september|october|november|december)[\\s][\\d]{1,2}[a-z][a-z](?:\\s[\\d]{2,4})|[\\d][\\d]\\w?\\w?\\sof\\s(?:january|february|march|april|may|june|july|august|september|october|november|december)(?:\\s[\\d]{2,4})?|(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s\\d\\d?\\w?\\w?,?(?:\\s\\d{2,4})?)\")\n",
    "\n",
    "def cleandates(datastring): \n",
    "\n",
    "    datastring = date_pattern.sub(\"date\", datastring.lower())\n",
    "    return datastring\n",
    "\n",
    "\n",
    "data['content'] = data['content'].apply(cleandates)\n",
    "data['content'] = [clean(entry,\n",
    "                              lower=True,\n",
    "                              no_line_breaks=True,\n",
    "                              no_emails=True,\n",
    "                              no_urls=True,\n",
    "                              no_numbers=True,\n",
    "                               lang=\"en\",\n",
    "                               replace_with_number=\"num\",\n",
    "                               replace_with_email=\"email\",\n",
    "                               replace_with_url=\"url\"\n",
    "                              ) for entry in data['content']]\n",
    "data['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "tokens = list(itertools.chain.from_iterable(data['content'].apply(nltk.word_tokenize)))\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords and compute reduction of vocabulary rate after having removed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction rate of removing stopwords: 0.007994186046511587\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set(tokens)\n",
    "tokens_no_stopwords = [word for word in tokens if word not in stopwords]\n",
    "vocabulary_no_stopwords = set(tokens_no_stopwords)\n",
    "print(\"Reduction rate of removing stopwords: \" + str(1 - len(vocabulary_no_stopwords) / len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem words and compute reduction rate of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction rate of stemming: 0.315995115995116\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "vocabulary_stem = set([stemmer.stem(word) for word in vocabulary_no_stopwords])\n",
    "print(\"Reduction rate of stemming: \" + str(1 - len(vocabulary_stem)/len(vocabulary_no_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 'NN'), ('trump', 'NN'), ('was', 'VBD'), ('president', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"donald trump was president\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 'TO'), ('trump', 'VB'), ('is', 'VBZ'), ('to', 'TO'), ('...', ':')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = nltk.word_tokenize(\"to trump is to ...\")\n",
    "nltk.pos_tag(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32cba0a30aa1fb672f7e284b7d7fddbbf2a1ff8f1469767a496c4b057944b499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
