{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get subset of FakeNewCorpus data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# Import 250 articles as pandas df\n",
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv', dtype={\"content\": \"string\"}, index_col=0) \n",
    "raw_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform basic cleaning: remove urls, dates, numbers, emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data250 = raw_data.copy(deep = True) # Create a deep copy of dataframe, before modifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import clean_dataframe\n",
    "clean_dataframe(data250)\n",
    "data250['content'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text\n",
    "Remove stopwords and compute reduction of vocabulary rate after having removed them\n",
    "Stem words and compute reduction rate of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import preprocess\n",
    "preprocess(data250)\n",
    "data250['content']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define functions that can visualize our dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import pair_keyword_type, scatterplot_keyword_type\n",
    "scatterplot_keyword_type(\"num\", data250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import boxplot_keyword_type\n",
    "boxplot_keyword_type('trump', data250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import barplot_keyword_type\n",
    "barplot_keyword_type('num', data250) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM HERE DOWN CODE HASN*T BEEN STRUCTURED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subset of full dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyarrow.feather as feather\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "import pyarrow.feather as feather\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"data/news_cleaned_2018_02_13-1.csv\", on_bad_lines='skip', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a sample consisting of 10% of the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_data_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make sure to delete the full data set from memory such that we can save our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del full_data\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we save the sample to csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_sample.to_csv(\"data/sample.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting The Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typedict = {}\n",
    "for i in data['type']: \n",
    "    if str(i) in typedict:\n",
    "        typedict[str(i)] +=1 \n",
    "    else: \n",
    "        typedict[str(i)] =1 \n",
    "typedict\n",
    "typedictperc = typedict.copy()\n",
    "#laver et nyt dictionairy som viser det i procenttal\n",
    "for i in typedictperc:\n",
    "    typedictperc[i] = (typedictperc[i]/(len(data)))*100\n",
    "\n",
    "typedictperc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allArticlesCount = 928083 + 146080 + 1300444 + 905981 + 144939 + 117374 + 292201 + 2435471 + 319830 + 1920139\n",
    "#Jeg henter data from README.md om hvor mange artikler af hver type, der er i det fulde datasæt\n",
    "realtypeperc = {\"reliable\": (1920139/allArticlesCount)*100, \"political\": (2435471/allArticlesCount)*100, \"bias\": (1300444/allArticlesCount)*100, \"Satire\": (146080/allArticlesCount)*100, \"fake\": (928083/allArticlesCount)*100, \"conspiracy\":(905981/allArticlesCount)*100, \"unreliable\": (319830/allArticlesCount)*100, \"clickbait\":(292201/allArticlesCount)*100, \"junksci\":(144939/allArticlesCount)*100 , \"hate\":(117374/allArticlesCount)*100}\n",
    "realtypeperc\n",
    "#jeg præsenterer det som procenter\n",
    "#ved sammenligning er det tydeligt, at Mikkels forkortede datasæt ikke har markant anderledes proportion ift. artikeltyper, end det fulde datasæt\n",
    "#Største forskel er at der er tilføjet nye typer af artikler som rumor til datasættet, siden README blev skrevet. \n",
    "#Vi kan bruge dette til at forsikre os selv om at vores forkortede datasæt er repræsentativt for det fulde datasæt. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Sample For Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we preprocess there are some articles we can remove from the data set. We also change the labels so that\n",
    "they are either 'reliable' or 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_dataframe(df): \n",
    "    # set all lables to either 'reliable' or 'fake' using the rules below\n",
    "    df.type = df.type.replace({'political': 'reliable', 'junksci': 'fake', 'bias' : 'fake', 'satire': 'fake', 'conspiracy': 'fake', 'rumor': 'fake', 'unreliable' : 'fake', 'clickbait': 'fake', 'hate': 'fake'})\n",
    "    # there are some junk labels due to bad formatting, so we remove those\n",
    "    df = df[(df.type == 'reliable') |(df.type == 'fake')]\n",
    "    # remove all NaN types\n",
    "    df = df[df.type.notnull()]\n",
    "    # remove all NaN content\n",
    "    df = df[df.content.notnull()]\n",
    "    # remove duplicates. Some entries have simmilar content, like a notification from Google Plus\n",
    "    # or a message saying that the TOR browser needs to be installed in order view the article.\n",
    "    df = df.drop_duplicates(subset = 'content', keep = 'last')\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_sample = structure_dataframe(full_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelperc(df): \n",
    "    labeldict = {}\n",
    "    for i in df.type: \n",
    "        if i in labeldict: \n",
    "            labeldict[i] +=1\n",
    "        else: \n",
    "            labeldict[i] = 1\n",
    "    for i in labeldict: \n",
    "        labeldict[i] = labeldict[i]/len(df)*100\n",
    "    return labeldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelperc(full_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_sample.to_csv(\"data/sample_STRUCTURED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the larger sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we preprocess the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"sample_preprocessed_ver_17_03_22\"\n",
    "for chunck in pd.read_csv(\"data/sample_STRUCTURED.csv\", chunksize=10000):\n",
    "    clean_dataframe(chunck)\n",
    "    preprocess(chunck)\n",
    "    start = time.time()\n",
    "    chunck.to_csv(file_name, mode='a')\n",
    "    end = time.time()\n",
    "    print(\"writing to csv took \" + str(end - start) + \" seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d82b2e002de3f5f2748faab7eed39a54b7eae736eec0eed65c518ce11052f6ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
