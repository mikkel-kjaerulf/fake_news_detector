{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get subset of FakeNewCorpus data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/church-congregation-brings-gift...</td>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Church Congregation Brings Gift to Waitresses ...</td>\n",
       "      <td>Ruth Harris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/awakening-start-here/...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>Zurich Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700</td>\n",
       "      <td>cnnnext.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://www.cnnnext.com/video/18526/never-hike-...</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Never Hike Alone - A Friday the 13th Fan Film ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/elusive-alien-of-the-sea-caught...</td>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Elusive ‘Alien Of The Sea ‘ Caught By Scientis...</td>\n",
       "      <td>Alexander Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>791</td>\n",
       "      <td>bipartisanreport.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>http://bipartisanreport.com/2018/01/21/trumps-...</td>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Trump’s Genius Poll Is Complete &amp; The Results ...</td>\n",
       "      <td>Gloria Christie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                domain        type  \\\n",
       "0  141               awm.com  unreliable   \n",
       "1  256     beforeitsnews.com        fake   \n",
       "2  700           cnnnext.com  unreliable   \n",
       "3  768               awm.com  unreliable   \n",
       "4  791  bipartisanreport.com   clickbait   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://awm.com/church-congregation-brings-gift...   \n",
       "1  http://beforeitsnews.com/awakening-start-here/...   \n",
       "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
       "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
       "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sometimes the power of Christmas will make you...   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3  When a rare shark was caught, scientists were ...   \n",
       "4  Donald Trump has the unnerving ability to abil...   \n",
       "\n",
       "                   scraped_at                 inserted_at  \\\n",
       "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                   updated_at  \\\n",
       "0  2018-02-02 01:19:41.756664   \n",
       "1  2018-02-02 01:19:41.756664   \n",
       "2  2018-02-02 01:19:41.756664   \n",
       "3  2018-02-02 01:19:41.756664   \n",
       "4  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                               title          authors  \\\n",
       "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
       "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
       "3  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
       "4  Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
       "\n",
       "   keywords meta_keywords                                   meta_description  \\\n",
       "0       NaN          ['']                                                NaN   \n",
       "1       NaN          ['']                                                NaN   \n",
       "2       NaN          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
       "3       NaN          ['']                                                NaN   \n",
       "4       NaN          ['']                                                NaN   \n",
       "\n",
       "  tags  summary  \n",
       "0  NaN      NaN  \n",
       "1  NaN      NaN  \n",
       "2  NaN      NaN  \n",
       "3  NaN      NaN  \n",
       "4  NaN      NaN  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv',index_col=0)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform basic cleaning: remove urls, dates, numbers, emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sometimes the power of christmas will make you...\n",
       "1    awakening of num strands of dna - \"reconnectin...\n",
       "2    never hike alone: a friday the 13th fan film u...\n",
       "3    when a rare shark was caught, scientists were ...\n",
       "4    donald trump has the unnerving ability to abil...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw_data.copy(deep = True)\n",
    "pattern = re.compile(r\"([\\d]{1,2}[\\/|\\-][\\d]{1,2}(?:[\\/|\\-][\\d]{2,4})?|[\\d]{2,4}[\\/|\\-][\\d]{1,2}[\\/|\\-][\\d]{1,2}|(?:january|february|march|april|may|june|july|august|september|october|november|december)[\\s][\\d]{1,2}[a-z][a-z](?:\\s[\\d]{2,4})|[\\d][\\d]\\w?\\w?\\sof\\s(?:january|february|march|april|may|june|july|august|september|october|november|december)(?:\\s[\\d]{2,4})?|(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s\\d\\d?\\w?\\w?,?(?:\\s\\d{2,4})?)\")\n",
    "\n",
    "def clean_string(s):\n",
    "    s1 = pattern.sub(\"date\", s.lower())\n",
    "    s2 = clean(s1, lower=True,\n",
    "                no_line_breaks=True,\n",
    "                no_emails=True,\n",
    "                no_urls=True,\n",
    "                no_numbers=True,\n",
    "                lang=\"en\",\n",
    "                replace_with_number=\"num\",\n",
    "                replace_with_email=\"email\",\n",
    "                replace_with_url=\"url\")\n",
    "    return s2\n",
    "\n",
    "data['content'] = data['content'].apply(clean_string)           \n",
    "data['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "tokens = list(itertools.chain.from_iterable(data['content'].apply(nltk.word_tokenize)))\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords and compute reduction of vocabulary rate after having removed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction rate of removing stopwords: 0.007994186046511587\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set(tokens)\n",
    "tokens_no_stopwords = [word for word in tokens if word not in stopwords]\n",
    "vocabulary_no_stopwords = set(tokens_no_stopwords)\n",
    "print(\"Reduction rate of removing stopwords: \" + str(1 - len(vocabulary_no_stopwords) / len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem words and compute reduction rate of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction rate of stemming: 0.315995115995116\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "vocabulary_stem = set([stemmer.stem(word) for word in vocabulary_no_stopwords])\n",
    "print(\"Reduction rate of stemming: \" + str(1 - len(vocabulary_stem)/len(vocabulary_no_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [sometimes, the, power, of, christmas, will, m...\n",
      "1      [awakening, of, num, strands, of, dna, -, ``, ...\n",
      "2      [never, hike, alone, :, a, friday, the, 13th, ...\n",
      "3      [when, a, rare, shark, was, caught, ,, scienti...\n",
      "4      [donald, trump, has, the, unnerving, ability, ...\n",
      "                             ...                        \n",
      "245    [prison, for, rahm, ,, god, 's, work, and, man...\n",
      "246    [num, useful, items, for, your, tiny, home, he...\n",
      "247    [former, cia, director, michael, hayden, said,...\n",
      "248    [antonio, sabato, jr., says, hollywood, 's, li...\n",
      "249    [former, u.s., president, bill, clinton, on, m...\n",
      "Name: content, Length: 250, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' vocabulary_no_stopwords = set(tokens_no_stopwords)\\nprint(\"Reduction rate of removing stopwords: \" + str(1 - len(vocabulary_no_stopwords) / len(vocabulary)))\\nstemmer = PorterStemmer()\\nvocabulary_stem = set([stemmer.stem(word) for word in vocabulary_no_stopwords])\\nprint(\"Reduction rate of stemming: \" + str(1 - len(vocabulary_stem)/len(vocabulary_no_stopwords))) '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# this apparently works\n",
    "data['content'] = data['content'].apply(nltk.word_tokenize)\n",
    "print(data['content'])\n",
    "\n",
    "\"\"\" def preprocess(dataframe):\n",
    "    # tokenize content column\n",
    "    dataframe['content'] = [[word] for word in nltk.word_tokenize()]\n",
    "    d = stopwords.words('english')\n",
    "    # remove stopwords\n",
    "    for token_list in df['content']:\n",
    "        token_list = [token for token in token_list if token not in d] \"\"\"    \n",
    "\"\"\" vocabulary_no_stopwords = set(tokens_no_stopwords)\n",
    "print(\"Reduction rate of removing stopwords: \" + str(1 - len(vocabulary_no_stopwords) / len(vocabulary)))\n",
    "stemmer = PorterStemmer()\n",
    "vocabulary_stem = set([stemmer.stem(word) for word in vocabulary_no_stopwords])\n",
    "print(\"Reduction rate of stemming: \" + str(1 - len(vocabulary_stem)/len(vocabulary_no_stopwords))) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d82b2e002de3f5f2748faab7eed39a54b7eae736eec0eed65c518ce11052f6ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
